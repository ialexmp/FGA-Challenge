{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Face and Gestures Analysis: Face Identification Challenge\n",
    "\n",
    "Students:\n",
    "* Manuel Parma (255570)\n",
    "* Àlex Montoya (242873)\n",
    "* Marina Riba (229240)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up and imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from imageio.v2 import imread\n",
    "from scipy.io import loadmat\n",
    "import itertools\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ],
   "metadata": {
    "id": "pTJbjAjH4cyI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face detection algorithm from Lab 1\n",
    "\n",
    "We are using the Viola Jones algorithm we fine-tuned during Lab 1 in order to crop input images to only the faces. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# face recognition code\n",
    "classifier_file = os.path.dirname(cv.__file__) + \"/data/haarcascade_frontalface_alt.xml\"\n",
    "face_cascade = cv.CascadeClassifier(classifier_file)\n",
    "\n",
    "def face_detection(img, scaleFactor=1.1, minNeighbors=6, minSize=[100, 100]):\n",
    "    \"\"\"\n",
    "    Method for detecting faces in an image using the Viola-Jones algorithm.\n",
    "    :param img: Image data to detect the faces on.\n",
    "    :return: Cropped image on face, None if no face is detected\n",
    "    \"\"\"\n",
    "    if len(img.shape) == 3:\n",
    "      # convert to grey image\n",
    "      gray_image = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "      gray_image = img\n",
    "\n",
    "    # we run multiple tests for the three parameters and got the best performance\n",
    "    # (approx. 85% accuracy) with these values.\n",
    "    faces = face_cascade.detectMultiScale(gray_image,\n",
    "                                          scaleFactor=scaleFactor,\n",
    "                                          minNeighbors=minNeighbors,\n",
    "                                          minSize=minSize)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "      return None\n",
    "\n",
    "    # we keep the biggest bounding box\n",
    "    if len(faces) > 1:\n",
    "        faces = sorted(faces, key=lambda rect: rect[2] * rect[3], reverse=True)\n",
    "        faces = faces[:1]\n",
    "\n",
    "    # convert to coordinates\n",
    "    x, y, w, h = faces[0]\n",
    "    return img[y:y+h, x:x+w]"
   ],
   "metadata": {
    "id": "aRqsw0ZT4fOg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Augmentation and Cleaning"
   ],
   "metadata": {
    "id": "2nUe59gdULgL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "AGC_Challenge3_TRAINING = loadmat(\"AGC_Challenge3_Training.mat\")\n",
    "AGC_Challenge3_TRAINING = np.squeeze(AGC_Challenge3_TRAINING['AGC_Challenge3_TRAINING'])\n",
    "\n",
    "imageName = AGC_Challenge3_TRAINING['imageName']\n",
    "imageName = list(itertools.chain.from_iterable(imageName))\n",
    "\n",
    "ids = list(AGC_Challenge3_TRAINING['id'])\n",
    "ids = np.concatenate(ids).ravel().tolist()"
   ],
   "metadata": {
    "id": "FwEuIFfhULL1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def group_by_labels(images, labels):\n",
    "    # we will group by labels\n",
    "    output = dict()\n",
    "    \n",
    "    for imagefile, label in zip(images, labels):\n",
    "      if label not in output.keys():\n",
    "        output[label] = list()\n",
    "      output[label].append(imagefile)\n",
    "        \n",
    "    return output\n",
    "\n",
    "label_to_images = group_by_labels(imageName, ids)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rWJDqRuvX4iW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708256215463,
     "user_tz": -60,
     "elapsed": 11,
     "user": {
      "displayName": "MANUEL FÉLIX PARMA",
      "userId": "06179278285013016664"
     }
    },
    "outputId": "71958503-95ce-490c-9ffb-04a4539aea50"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This next code divided the TRAINING dataset in folders according to their labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# First we will group files by class\n",
    "PERFORM_CLASSIFICATION = False\n",
    "\n",
    "root_dir = \"./TRAINING/\"\n",
    "output_dir = \"./TRAINING_CLASS/\"\n",
    "\n",
    "if PERFORM_CLASSIFICATION:\n",
    "  if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "      \n",
    "  for label, images in label_to_images.items():\n",
    "    class_dir = os.path.join(output_dir, str(label))\n",
    "\n",
    "    if not os.path.exists(class_dir):\n",
    "      os.makedirs(class_dir)\n",
    "\n",
    "    if int(label) == -1:\n",
    "        continue\n",
    "\n",
    "    print(f\"Moving class {label}\")\n",
    "\n",
    "    for image_file in images:\n",
    "      # Open the image and copy it\n",
    "      image_path = os.path.join(root_dir, image_file)\n",
    "      \n",
    "      if int(label) == -1:\n",
    "         # keep only faces from this class\n",
    "         image = imread(image_path)\n",
    "         if face_detection(image) is None:\n",
    "            continue\n",
    "      \n",
    "      image = Image.open(image_path)\n",
    "      image.save(f\"{class_dir}/{image_file}\")\n"
   ],
   "metadata": {
    "id": "Q4vZk3JoY2bn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the training data (and after some failed first attempts to build a model), we realized that the dataset is heavily unbalanced with \"unknown\" or \"unidentified\" images (those with label -1).\n",
    "\n",
    "*   Identified faces: 480\n",
    "*   Unknown (label -1): 720\n",
    "\n",
    "We realized that the data was insufficient for most of the identities (most had 4 images only). We decided to increase the dataset downloading images from Google Images for each of the classes. Now, as the dataset has increased in size, we run the face detection algorithm to discard those where a face was not detected.\n",
    "\n",
    "Additionally, we added fake faces images (from \"thispersondoesnotexist.com\") as the \"-1\"  label, and some generic images, and run it through Viola-Jones again. This was in order to make the CNN predict unknown identities and the case of no faces on the image.\n",
    "\n",
    "We also removed the images from the TRAINING directory provided by the teachers, so this data can be used for unseen testing with the challenge script."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PERFORM_TRANSFORMATION = False\n",
    "\n",
    "root_dir = \"./TRAINING_CLASS/\"\n",
    "output_dir = \"./TRAINING_AUGMENTED/\"\n",
    "\n",
    "if PERFORM_TRANSFORMATION:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get the classes of the dataset\n",
    "    classes = os.listdir(root_dir)\n",
    "    classes.sort()\n",
    "    \n",
    "    for class_label in classes:\n",
    "        root_class_dir = os.path.join(root_dir, str(class_label))\n",
    "        output_class_dir = os.path.join(output_dir, str(class_label))\n",
    "        \n",
    "        if not os.path.exists(output_class_dir):\n",
    "            os.makedirs(output_class_dir)\n",
    "        \n",
    "        image_files = os.listdir(root_class_dir)\n",
    "        \n",
    "        faces_count = 0\n",
    "        idx = 0\n",
    "        \n",
    "        for idx, imagefile in enumerate(image_files):\n",
    "            image_path = os.path.join(root_class_dir, imagefile)\n",
    "            _, image_extension = os.path.splitext(image_path)\n",
    "\n",
    "            image = imread(image_path)\n",
    "            \n",
    "            face_detected = face_detection(image)\n",
    "            \n",
    "            if face_detected is None:\n",
    "                continue\n",
    "                \n",
    "            out_path = os.path.join(output_class_dir, f\"{idx}{image_extension}\")\n",
    "            image = Image.fromarray(face_detected).convert('RGB')\n",
    "            faces_count += 1\n",
    "            \n",
    "            image.save(out_path)\n",
    "                        \n",
    "        print(f\"Transformed class {class_label}: {faces_count}/{idx} faces\")\n",
    "            \n",
    "print(\"FINISHED!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loader"
   ],
   "metadata": {
    "id": "2ATaZRbBICfy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_dir = \"./TRAINING_AUGMENTED/\"\n",
    "\n",
    "# \"translations\" of labels to trainable numbers\n",
    "labels_to_number = {}\n",
    "number_to_labels = {}\n",
    "\n",
    "# Get the classes of the dataset\n",
    "classes = os.listdir(training_dir)\n",
    "classes.sort()\n",
    "\n",
    "imagefiles = []\n",
    "labels = []\n",
    "\n",
    "for idx, class_name in enumerate(classes):\n",
    "  # Get the image paths of the current class\n",
    "  class_dir = os.path.join(training_dir, class_name)\n",
    "  class_files = os.listdir(class_dir)\n",
    "\n",
    "  # Create the dictionaries\n",
    "  labels_to_number[class_name] = idx\n",
    "  number_to_labels[idx] = class_name\n",
    "\n",
    "  for image_file in class_files:\n",
    "      image_path = os.path.join(class_dir, image_file)\n",
    "\n",
    "      if not os.path.isdir(image_path):\n",
    "          imagefiles.append(image_path)\n",
    "          labels.append(idx)\n",
    "\n",
    "# split into training and validation\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(imagefiles, labels, test_size=0.2, random_state=42)\n",
    "print(\"training size:\", len(data_train))\n",
    "print(\"validation size:\", len(data_val))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcM6HuqsKD5c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708256227411,
     "user_tz": -60,
     "elapsed": 11957,
     "user": {
      "displayName": "MANUEL FÉLIX PARMA",
      "userId": "06179278285013016664"
     }
    },
    "outputId": "29e70802-2878-4ddb-c856-b1ac09148c14"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each input image, we apply the different augmentations we specified below separately. That is, each image will appear 6 times in the training data loader: its original, and 5 different transformations. This helped us to increment the training size very easily."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# transformations for normalizing input\n",
    "tr = tf.Compose([\n",
    "    tf.RandomHorizontalFlip(),\n",
    "    tf.ToTensor(),\n",
    "    tf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "resize = tf.Resize((224, 224))\n",
    "\n",
    "augmentations = [\n",
    "    tf.RandomRotation(degrees=(-30, 30)), \n",
    "    tf.GaussianBlur(kernel_size=(5, 9)), \n",
    "    tf.Compose([tf.Resize((50, 50)), tf.Resize((224, 224)),]),\n",
    "    tf.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "    tf.Compose([tf.RandomCrop(size=(170, 170)), tf.Resize((224, 224)),]),\n",
    "]\n",
    "\n",
    "# Data loader for images and labels/identities\n",
    "class FacesDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, images_names, images_labels, augment_data=False):\n",
    "        self.images_names = images_names\n",
    "        self.labels = images_labels\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        # list of (image path, label, transforms to apply)\n",
    "        self.__data = []\n",
    "\n",
    "        for idx in range(len(images_names)):\n",
    "          self.__load_image(idx)\n",
    "\n",
    "    def __load_image(self, index):\n",
    "        image_path = self.images_names[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # keep original image\n",
    "        self.__data.append((\n",
    "            image_path, label, None\n",
    "        ))\n",
    "        \n",
    "        if self.augment_data:\n",
    "            for aug in augmentations:\n",
    "                self.__data.append((\n",
    "                    image_path, label, aug\n",
    "                ))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, label, aug = self.__data[index]\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Apply the preprocessing\n",
    "        image = resize(image)\n",
    "                \n",
    "        if aug is not None:\n",
    "            image = aug(image)\n",
    "        \n",
    "        image = tr(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__data)"
   ],
   "metadata": {
    "id": "wfNPenTkIID5"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied the transformations to the training dataset, but not to the validation dataset (to make sure the model is learning the correct features)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Training Dataset\n",
    "print(\"Loading training dataset...\")\n",
    "train_dataset = FacesDataset(data_train, labels_train, augment_data=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Training size: {len(train_dataset)}\")\n",
    "\n",
    "# Validation Dataset\n",
    "print(\"Loading validation dataset...\")\n",
    "val_dataset = FacesDataset(data_val, labels_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Training size: {len(val_dataset)}\")\n"
   ],
   "metadata": {
    "id": "pGSEtbEFLQ1J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN"
   ],
   "metadata": {
    "id": "TqIEjosJFDEZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below are some of the models we used, but as these were modified after each training sessions, not all of them are included. VGGSimple5 (which came after 5 iterations of VGGSimple) was the model that best performed for us."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Simple CNN similar to VGG (Source: Deep Learning course 2023 at UPF)\n",
    "class VGGSimple(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "\n",
    "        super(VGGSimple, self).__init__()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv22 = nn.Conv2d(128,128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.maxpool= nn.MaxPool2d(kernel_size=5, stride=5)\n",
    "\n",
    "        self.fc = nn.Linear(8*8*128, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.relu(self.conv11(x))\n",
    "        out = self.relu(self.conv12(out))\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.relu(self.conv21(out))\n",
    "        out = self.relu(self.conv22(out))\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VGGSimple5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "\n",
    "        super(VGGSimple5, self).__init__()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv20 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv22 = nn.Conv2d(128,128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(8*8*128, 80)\n",
    "        self.fc2 = nn.Linear(80, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.relu(self.conv11(x))\n",
    "        out = self.relu(self.conv12(out))\n",
    "        out = self.relu(self.conv13(out))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.relu(self.conv20(out))\n",
    "        out = self.relu(self.conv21(out))\n",
    "        out = self.relu(self.conv22(out))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, \n",
    "                                   groups=in_channels, bias=bias, padding=1)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, \n",
    "                                   kernel_size=1, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VGGSeparableNew(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "\n",
    "        super(VGGSeparableNew, self).__init__()\n",
    "\n",
    "        self.conv11 = SeparableConv2d(3, 64, kernel_size=3)\n",
    "        self.conv12 = SeparableConv2d(64, 64, kernel_size=3)\n",
    "        self.conv13 = SeparableConv2d(64, 64, kernel_size=3)\n",
    "\n",
    "        self.conv21 = SeparableConv2d(64, 128, kernel_size=3)\n",
    "        self.conv22 = SeparableConv2d(128, 128, kernel_size=3)\n",
    "        self.conv23 = SeparableConv2d(128, 128, kernel_size=3)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(8*8*128, 110)\n",
    "        self.fc2 = nn.Linear(110, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.relu(self.conv11(x))\n",
    "        out = self.relu(self.conv12(out))\n",
    "        out = self.relu(self.conv13(out))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.relu(self.conv21(out))\n",
    "        out = self.relu(self.conv22(out))\n",
    "        out = self.relu(self.conv23(out))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "pIJVuAdQP3GT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Functions from Deep Learning course 2023 at UPF\n",
    "\n",
    "# Train function\n",
    "def train(CNN, train_loader, val_loader, optimizer, num_epochs=5, model_name='model.ckpt', device='cpu', results_path = './results/'):\n",
    "    CNN.train()  # Set the model in train mode\n",
    "    total_step = len(train_loader)\n",
    "    losses_list = []\n",
    "    val_losses_list = []\n",
    "    val_acc_list = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        loss_avg = 0\n",
    "        nBatches = 0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = CNN(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_avg += loss.cpu().item()\n",
    "            nBatches += 1\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, num_epochs, i + 1, total_step, loss_avg / nBatches))\n",
    "\n",
    "        print('Epoch [{}/{}], Training Loss: {:.4f}'\n",
    "              .format(epoch + 1, num_epochs, loss_avg / nBatches))\n",
    "        losses_list.append(loss_avg / nBatches)\n",
    "\n",
    "        # Validation\n",
    "        CNN.eval()  # Set the model in evaluation mode\n",
    "        val_loss_avg = 0\n",
    "        val_nBatches = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "                outputs = CNN(images)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                val_loss_avg += val_loss.cpu().item()\n",
    "                val_nBatches += 1\n",
    "\n",
    "        val_loss_avg /= val_nBatches\n",
    "        print('Epoch [{}/{}], Validation Loss: {:.4f}'.format(epoch + 1, num_epochs, val_loss_avg))\n",
    "        val_losses_list.append(val_loss_avg)\n",
    "        \n",
    "        val_acc = test(CNN, val_loader)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print(f'Validation Accuracy: {val_acc}')\n",
    "\n",
    "        CNN.train()  # Set the model back to train mode\n",
    "\n",
    "        # Check if the results directory exists, or create it\n",
    "        if not os.path.exists(results_path):\n",
    "          os.makedirs(results_path)\n",
    "\n",
    "        torch.save(CNN.state_dict(), results_path + f\"epoch_{epoch}_\" + model_name)\n",
    "\n",
    "    return losses_list, val_losses_list, val_acc_list"
   ],
   "metadata": {
    "id": "uarMpl6TP4bi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To calculate the validation accuracy, we used the F1-Score method provided by the course for the challenge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def CHALL_AGC_ComputeRecognScores(auto_ids, true_ids):\n",
    "    #   Compute face recognition score\n",
    "    #\n",
    "    #   INPUTS\n",
    "    #     - AutomSTR: The results of the automatic face\n",
    "    #     recognition algorithm, stored as an integer\n",
    "    #\n",
    "    #     - AGC_Challenge_STR: The ground truth ids\n",
    "    #\n",
    "    #   OUTPUT\n",
    "    #     - FR_score:     The final recognition score\n",
    "    #\n",
    "    #   --------------------------------------------------------------------\n",
    "    #   AGC Challenge\n",
    "    #   Universitat Pompeu Fabra\n",
    "    #\n",
    "\n",
    "    if len(auto_ids) != len(true_ids):\n",
    "        assert ('Inputs must be of the same len');\n",
    "\n",
    "    # convert to teacher's values\n",
    "    auto_ids = [int(number_to_labels[value]) for value in auto_ids]\n",
    "    true_ids = [int(number_to_labels[value]) for value in true_ids]\n",
    "    # convert 0 class to -1 (they represent the same)\n",
    "    auto_ids = [value if value != 0 else -1 for value in auto_ids]\n",
    "    true_ids = [value if value != 0 else -1 for value in true_ids]\n",
    "\n",
    "    f_beta = 1\n",
    "    res_list = list(filter(lambda x: true_ids[x] != -1, range(len(true_ids))))\n",
    "\n",
    "    nTP = len([i for i in res_list if auto_ids[i] == true_ids[i]])\n",
    "\n",
    "    res_list = list(filter(lambda x: auto_ids[x] != -1, range(len(auto_ids))))\n",
    "\n",
    "    nFP = len([i for i in res_list if auto_ids[i] != true_ids[i]])\n",
    "\n",
    "    res_list_auto_ids = list(filter(lambda x: auto_ids[x] == -1, range(len(auto_ids))))\n",
    "    res_list_true_ids = list(filter(lambda x: true_ids[x] != -1, range(len(true_ids))))\n",
    "\n",
    "    nFN = len(set(res_list_auto_ids).intersection(res_list_true_ids))\n",
    "\n",
    "    FR_score = (1 + f_beta ** 2) * nTP / ((1 + f_beta ** 2) * nTP + f_beta ** 2 * nFN + nFP)\n",
    "\n",
    "    return FR_score\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test(CNN, test_loader):\n",
    "  CNN.eval()\n",
    "  with torch.no_grad():\n",
    "        real_ids = []\n",
    "        predicted_ids = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # get network predictions\n",
    "            outputs = CNN(images)\n",
    "\n",
    "            # get predicted class\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            real_ids.extend(labels.cpu().tolist())\n",
    "            predicted_ids.extend(predicted.cpu().tolist())\n",
    "\n",
    "  # return accuracy\n",
    "  return CHALL_AGC_ComputeRecognScores(predicted_ids, real_ids) * 100"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_loss(loss_hist, val_loss_hist, model_name, results_path):\n",
    "    # visualize the results\n",
    "    plt.plot(loss_hist, '-.r', linewidth=1.0, label='train_loss')\n",
    "    plt.plot(val_loss_hist,'-b', linewidth=1.0, label='val_loss')\n",
    "    plt.xlabel('train step', fontsize=14)\n",
    "    plt.ylabel('loss', fontsize=14)\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(results_path + f\"Loss_{model_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "  \n",
    "def plot_acc(acc_hist, model_name, results_path):\n",
    "    plt.plot(acc_hist, '-b', linewidth=1.0, label='validation')\n",
    "    plt.xlabel('train step', fontsize=14)\n",
    "    plt.ylabel('accuracy', fontsize=14)\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.savefig(results_path + f\"Accuracy_{model_name}.png\")\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "    "
   ],
   "metadata": {
    "id": "vmf25DuQfnm2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    # count the parameters of the model\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# here we can specify which models and learning rates to train\n",
    "training_combinations = [(VGGSimple5, 0.01)]\n",
    "\n",
    "num_epochs=25\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "num_classes = len(classes)\n",
    "print(f\"Num of classes: {num_classes}\")\n",
    "\n",
    "for model_class, lr in training_combinations:\n",
    "    # free cuda memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    my_model = model_class(num_classes)\n",
    "    model_name = model_class.__name__ + \"_SGD\"\n",
    "    results_path = f'./results/{model_name}_{lr}/'\n",
    "\n",
    "    print(\"Model:\", model_name)\n",
    "    print(f\"Model parameters: {count_parameters(my_model)}\")\n",
    "    print(\"Learning rate:\", lr)\n",
    "    print(\"Results folder:\", results_path)\n",
    "    \n",
    "    # optimizer = torch.optim.Adam(my_model.parameters(), lr = lr, weight_decay=lr/10)\n",
    "    optimizer = torch.optim.SGD(my_model.parameters(), lr = lr, momentum=0.9, weight_decay=lr/10)\n",
    "\n",
    "    model = my_model.to(device)\n",
    "        \n",
    "    if not os.path.exists(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    \n",
    "    losses_list, val_losses_list, val_acc_list = train(model, train_loader, val_loader, optimizer,\n",
    "            num_epochs=num_epochs, model_name=f'{model_name}.ckpt', device=device,\n",
    "            results_path=results_path)\n",
    "    \n",
    "    plot_loss(losses_list, val_losses_list, model_name, results_path=results_path)\n",
    "    plot_acc(val_acc_list, model_name, results_path=results_path)\n",
    "    \n",
    "    output = dict()\n",
    "\n",
    "    output[\"labels_to_number\"] = labels_to_number\n",
    "    output[\"number_to_labels\"] = number_to_labels\n",
    "    \n",
    "    with open(results_path + \"label_dicts.pk\", \"wb\") as f:\n",
    "        pickle.dump(output, f)\n",
    "        \n",
    "    print(\"-\" * 50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ]
}
